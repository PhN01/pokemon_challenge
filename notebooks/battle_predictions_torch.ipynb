{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Battle Predictions\n",
    "\n",
    "The following notebook is intended to be used to train deep architectures on the task of predicting the battle outcome of a fight of two pokemons. This is part of the MCK Pokemon Hackathon challenge 2020.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Environmental setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f659c662df0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.utils.data as data_utils\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Read in data\n",
    "\n",
    "Before, we start with the analyses we will read in the preprocessed battle data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name_1</th>\n",
       "      <th>Level_1</th>\n",
       "      <th>Price_1</th>\n",
       "      <th>HP_1</th>\n",
       "      <th>Attack_1</th>\n",
       "      <th>Defense_1</th>\n",
       "      <th>Sp_Atk_1</th>\n",
       "      <th>Sp_Def_1</th>\n",
       "      <th>Speed_1</th>\n",
       "      <th>Legendary_1</th>\n",
       "      <th>...</th>\n",
       "      <th>Poison_2</th>\n",
       "      <th>Psychic_2</th>\n",
       "      <th>Rock_2</th>\n",
       "      <th>Water_2</th>\n",
       "      <th>Night</th>\n",
       "      <th>Rain</th>\n",
       "      <th>Sunshine</th>\n",
       "      <th>Unknown</th>\n",
       "      <th>Windy</th>\n",
       "      <th>HPPR_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Metapod</td>\n",
       "      <td>30</td>\n",
       "      <td>441</td>\n",
       "      <td>150</td>\n",
       "      <td>36</td>\n",
       "      <td>103</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Pinsir</td>\n",
       "      <td>36</td>\n",
       "      <td>1227</td>\n",
       "      <td>196</td>\n",
       "      <td>304</td>\n",
       "      <td>237</td>\n",
       "      <td>129</td>\n",
       "      <td>164</td>\n",
       "      <td>197</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Metapod</td>\n",
       "      <td>15</td>\n",
       "      <td>297</td>\n",
       "      <td>92</td>\n",
       "      <td>26</td>\n",
       "      <td>73</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Pinsir</td>\n",
       "      <td>40</td>\n",
       "      <td>1401</td>\n",
       "      <td>228</td>\n",
       "      <td>346</td>\n",
       "      <td>270</td>\n",
       "      <td>146</td>\n",
       "      <td>187</td>\n",
       "      <td>224</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.938596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Pinsir</td>\n",
       "      <td>12</td>\n",
       "      <td>634</td>\n",
       "      <td>91</td>\n",
       "      <td>157</td>\n",
       "      <td>124</td>\n",
       "      <td>70</td>\n",
       "      <td>86</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.450549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Name_1  Level_1  Price_1  HP_1  Attack_1  Defense_1  Sp_Atk_1  Sp_Def_1  \\\n",
       "0  Metapod       30      441   150        36        103        47        47   \n",
       "1   Pinsir       36     1227   196       304        237       129       164   \n",
       "2  Metapod       15      297    92        26         73        33        33   \n",
       "3   Pinsir       40     1401   228       346        270       146       187   \n",
       "4   Pinsir       12      634    91       157        124        70        86   \n",
       "\n",
       "   Speed_1  Legendary_1  ... Poison_2  Psychic_2  Rock_2  Water_2  Night  \\\n",
       "0       58            0  ...        0          0       0        0      0   \n",
       "1      197            0  ...        0          0       0        0      0   \n",
       "2       40            0  ...        0          0       0        0      0   \n",
       "3      224            0  ...        0          0       0        0      0   \n",
       "4      106            0  ...        0          0       0        0      0   \n",
       "\n",
       "   Rain  Sunshine  Unknown  Windy    HPPR_1  \n",
       "0     0         0        0      1  0.000000  \n",
       "1     0         0        1      0  0.000000  \n",
       "2     0         0        0      1  0.000000  \n",
       "3     0         0        1      0  0.938596  \n",
       "4     0         0        0      1  0.450549  \n",
       "\n",
       "[5 rows x 67 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "battles = pd.read_csv('../data/04_features/battles_preprocessed.csv')\n",
    "battles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will clean the data by putting the duplicate battles, which we will later add again to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2682071\n"
     ]
    }
   ],
   "source": [
    "aside = battles.loc[battles.duplicate_count==2,:].copy().drop('duplicate_count', axis=1)\n",
    "battles = battles.loc[battles.duplicate_count==1,:].reset_index(drop=True).drop('duplicate_count', axis=1)\n",
    "n_unique_battles = battles.shape[0]\n",
    "print(n_unique_battles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Data splitting\n",
    "\n",
    "After having read in the data, we will now split the data into a training, validation and test set. This will enable us assess different models for the task on the test set, while selecting the models during training using the validation score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 315557  206580 2373003 ...  165158 2548435  486191]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "idc = np.arange(n_unique_battles)\n",
    "shuffled_idc = np.random.permutation(idc)\n",
    "print(shuffled_idc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_test_split = [0.7, 0.2, 0.1]\n",
    "\n",
    "train_split_idx = int(n_unique_battles*train_val_test_split[0])\n",
    "val_split_idx = int(n_unique_battles*(train_val_test_split[0]+train_val_test_split[1]))\n",
    "\n",
    "train_battles = battles.iloc[shuffled_idc[:train_split_idx],:]\n",
    "val_battles = battles.iloc[shuffled_idc[train_split_idx:val_split_idx], :]\n",
    "test_battles = battles.iloc[shuffled_idc[val_split_idx:], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Level_1</th>\n",
       "      <th>Price_1</th>\n",
       "      <th>HP_1</th>\n",
       "      <th>Attack_1</th>\n",
       "      <th>Defense_1</th>\n",
       "      <th>Sp_Atk_1</th>\n",
       "      <th>Sp_Def_1</th>\n",
       "      <th>Speed_1</th>\n",
       "      <th>Legendary_1</th>\n",
       "      <th>Level_2</th>\n",
       "      <th>...</th>\n",
       "      <th>Poison_2</th>\n",
       "      <th>Psychic_2</th>\n",
       "      <th>Rock_2</th>\n",
       "      <th>Water_2</th>\n",
       "      <th>Night</th>\n",
       "      <th>Rain</th>\n",
       "      <th>Sunshine</th>\n",
       "      <th>Unknown</th>\n",
       "      <th>Windy</th>\n",
       "      <th>HPPR_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>1.877449e+06</td>\n",
       "      <td>1.877449e+06</td>\n",
       "      <td>1.877449e+06</td>\n",
       "      <td>1.877449e+06</td>\n",
       "      <td>1.877449e+06</td>\n",
       "      <td>1.877449e+06</td>\n",
       "      <td>1.877449e+06</td>\n",
       "      <td>1.877449e+06</td>\n",
       "      <td>1.877449e+06</td>\n",
       "      <td>1.877449e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.877449e+06</td>\n",
       "      <td>1.877449e+06</td>\n",
       "      <td>1.877449e+06</td>\n",
       "      <td>1.877449e+06</td>\n",
       "      <td>1.877449e+06</td>\n",
       "      <td>1.877449e+06</td>\n",
       "      <td>1877449.0</td>\n",
       "      <td>1.877449e+06</td>\n",
       "      <td>1.877449e+06</td>\n",
       "      <td>1.877449e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>5.000264e+01</td>\n",
       "      <td>1.780242e+03</td>\n",
       "      <td>3.329903e+02</td>\n",
       "      <td>3.044913e+02</td>\n",
       "      <td>2.873161e+02</td>\n",
       "      <td>2.837575e+02</td>\n",
       "      <td>2.810847e+02</td>\n",
       "      <td>2.906018e+02</td>\n",
       "      <td>2.067433e-02</td>\n",
       "      <td>4.998841e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.205146e-01</td>\n",
       "      <td>8.988207e-02</td>\n",
       "      <td>6.890627e-02</td>\n",
       "      <td>2.137097e-01</td>\n",
       "      <td>1.998792e-01</td>\n",
       "      <td>2.000693e-01</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.000081e-01</td>\n",
       "      <td>2.000433e-01</td>\n",
       "      <td>4.209084e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>2.857739e+01</td>\n",
       "      <td>1.328780e+03</td>\n",
       "      <td>2.620745e+02</td>\n",
       "      <td>2.511628e+02</td>\n",
       "      <td>2.429625e+02</td>\n",
       "      <td>2.436014e+02</td>\n",
       "      <td>2.299843e+02</td>\n",
       "      <td>2.423465e+02</td>\n",
       "      <td>1.422916e-01</td>\n",
       "      <td>2.857526e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>4.145938e-01</td>\n",
       "      <td>2.860128e-01</td>\n",
       "      <td>2.532947e-01</td>\n",
       "      <td>4.099243e-01</td>\n",
       "      <td>3.999095e-01</td>\n",
       "      <td>4.000521e-01</td>\n",
       "      <td>0.4</td>\n",
       "      <td>4.000062e-01</td>\n",
       "      <td>4.000325e-01</td>\n",
       "      <td>4.567580e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.950000e+02</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>1.500000e+01</td>\n",
       "      <td>2.000000e+01</td>\n",
       "      <td>1.500000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>2.500000e+01</td>\n",
       "      <td>7.290000e+02</td>\n",
       "      <td>1.310000e+02</td>\n",
       "      <td>1.190000e+02</td>\n",
       "      <td>1.110000e+02</td>\n",
       "      <td>1.090000e+02</td>\n",
       "      <td>1.100000e+02</td>\n",
       "      <td>1.130000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.500000e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>5.000000e+01</td>\n",
       "      <td>1.356000e+03</td>\n",
       "      <td>2.530000e+02</td>\n",
       "      <td>2.220000e+02</td>\n",
       "      <td>2.070000e+02</td>\n",
       "      <td>2.000000e+02</td>\n",
       "      <td>2.030000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.000000e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>7.500000e+01</td>\n",
       "      <td>2.509000e+03</td>\n",
       "      <td>4.720000e+02</td>\n",
       "      <td>4.210000e+02</td>\n",
       "      <td>3.920000e+02</td>\n",
       "      <td>3.820000e+02</td>\n",
       "      <td>3.840000e+02</td>\n",
       "      <td>3.960000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.500000e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>9.900000e+01</td>\n",
       "      <td>8.106000e+03</td>\n",
       "      <td>2.312000e+03</td>\n",
       "      <td>1.865000e+03</td>\n",
       "      <td>2.336000e+03</td>\n",
       "      <td>1.685000e+03</td>\n",
       "      <td>1.508000e+03</td>\n",
       "      <td>1.595000e+03</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>9.900000e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Level_1       Price_1          HP_1      Attack_1     Defense_1  \\\n",
       "count  1.877449e+06  1.877449e+06  1.877449e+06  1.877449e+06  1.877449e+06   \n",
       "mean   5.000264e+01  1.780242e+03  3.329903e+02  3.044913e+02  2.873161e+02   \n",
       "std    2.857739e+01  1.328780e+03  2.620745e+02  2.511628e+02  2.429625e+02   \n",
       "min    1.000000e+00  1.950000e+02  1.000000e+01  5.000000e+00  5.000000e+00   \n",
       "25%    2.500000e+01  7.290000e+02  1.310000e+02  1.190000e+02  1.110000e+02   \n",
       "50%    5.000000e+01  1.356000e+03  2.530000e+02  2.220000e+02  2.070000e+02   \n",
       "75%    7.500000e+01  2.509000e+03  4.720000e+02  4.210000e+02  3.920000e+02   \n",
       "max    9.900000e+01  8.106000e+03  2.312000e+03  1.865000e+03  2.336000e+03   \n",
       "\n",
       "           Sp_Atk_1      Sp_Def_1       Speed_1   Legendary_1       Level_2  \\\n",
       "count  1.877449e+06  1.877449e+06  1.877449e+06  1.877449e+06  1.877449e+06   \n",
       "mean   2.837575e+02  2.810847e+02  2.906018e+02  2.067433e-02  4.998841e+01   \n",
       "std    2.436014e+02  2.299843e+02  2.423465e+02  1.422916e-01  2.857526e+01   \n",
       "min    1.500000e+01  2.000000e+01  1.500000e+01  0.000000e+00  1.000000e+00   \n",
       "25%    1.090000e+02  1.100000e+02  1.130000e+02  0.000000e+00  2.500000e+01   \n",
       "50%    2.000000e+02  2.030000e+02  2.080000e+02  0.000000e+00  5.000000e+01   \n",
       "75%    3.820000e+02  3.840000e+02  3.960000e+02  0.000000e+00  7.500000e+01   \n",
       "max    1.685000e+03  1.508000e+03  1.595000e+03  1.000000e+00  9.900000e+01   \n",
       "\n",
       "       ...      Poison_2     Psychic_2        Rock_2       Water_2  \\\n",
       "count  ...  1.877449e+06  1.877449e+06  1.877449e+06  1.877449e+06   \n",
       "mean   ...  2.205146e-01  8.988207e-02  6.890627e-02  2.137097e-01   \n",
       "std    ...  4.145938e-01  2.860128e-01  2.532947e-01  4.099243e-01   \n",
       "min    ...  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%    ...  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "50%    ...  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "75%    ...  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "max    ...  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "\n",
       "              Night          Rain   Sunshine       Unknown         Windy  \\\n",
       "count  1.877449e+06  1.877449e+06  1877449.0  1.877449e+06  1.877449e+06   \n",
       "mean   1.998792e-01  2.000693e-01        0.2  2.000081e-01  2.000433e-01   \n",
       "std    3.999095e-01  4.000521e-01        0.4  4.000062e-01  4.000325e-01   \n",
       "min    0.000000e+00  0.000000e+00        0.0  0.000000e+00  0.000000e+00   \n",
       "25%    0.000000e+00  0.000000e+00        0.0  0.000000e+00  0.000000e+00   \n",
       "50%    0.000000e+00  0.000000e+00        0.0  0.000000e+00  0.000000e+00   \n",
       "75%    0.000000e+00  0.000000e+00        0.0  0.000000e+00  0.000000e+00   \n",
       "max    1.000000e+00  1.000000e+00        1.0  1.000000e+00  1.000000e+00   \n",
       "\n",
       "             HPPR_1  \n",
       "count  1.877449e+06  \n",
       "mean   4.209084e-01  \n",
       "std    4.567580e-01  \n",
       "min    0.000000e+00  \n",
       "25%    0.000000e+00  \n",
       "50%    0.000000e+00  \n",
       "75%    1.000000e+00  \n",
       "max    1.000000e+00  \n",
       "\n",
       "[8 rows x 63 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_battles.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Level_1</th>\n",
       "      <th>Price_1</th>\n",
       "      <th>HP_1</th>\n",
       "      <th>Attack_1</th>\n",
       "      <th>Defense_1</th>\n",
       "      <th>Sp_Atk_1</th>\n",
       "      <th>Sp_Def_1</th>\n",
       "      <th>Speed_1</th>\n",
       "      <th>Legendary_1</th>\n",
       "      <th>Level_2</th>\n",
       "      <th>...</th>\n",
       "      <th>Poison_2</th>\n",
       "      <th>Psychic_2</th>\n",
       "      <th>Rock_2</th>\n",
       "      <th>Water_2</th>\n",
       "      <th>Night</th>\n",
       "      <th>Rain</th>\n",
       "      <th>Sunshine</th>\n",
       "      <th>Unknown</th>\n",
       "      <th>Windy</th>\n",
       "      <th>HPPR_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>536414.000000</td>\n",
       "      <td>536414.000000</td>\n",
       "      <td>536414.000000</td>\n",
       "      <td>536414.000000</td>\n",
       "      <td>536414.000000</td>\n",
       "      <td>536414.000000</td>\n",
       "      <td>536414.000000</td>\n",
       "      <td>536414.000000</td>\n",
       "      <td>536414.000000</td>\n",
       "      <td>536414.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>536414.000000</td>\n",
       "      <td>536414.000000</td>\n",
       "      <td>536414.000000</td>\n",
       "      <td>536414.000000</td>\n",
       "      <td>536414.000000</td>\n",
       "      <td>536414.000000</td>\n",
       "      <td>536414.000000</td>\n",
       "      <td>536414.000000</td>\n",
       "      <td>536414.000000</td>\n",
       "      <td>536414.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>49.984814</td>\n",
       "      <td>1778.619337</td>\n",
       "      <td>332.488647</td>\n",
       "      <td>304.010127</td>\n",
       "      <td>287.065308</td>\n",
       "      <td>283.583793</td>\n",
       "      <td>280.939722</td>\n",
       "      <td>290.531740</td>\n",
       "      <td>0.020732</td>\n",
       "      <td>49.993829</td>\n",
       "      <td>...</td>\n",
       "      <td>0.220494</td>\n",
       "      <td>0.089155</td>\n",
       "      <td>0.069273</td>\n",
       "      <td>0.214295</td>\n",
       "      <td>0.200772</td>\n",
       "      <td>0.199510</td>\n",
       "      <td>0.199980</td>\n",
       "      <td>0.199536</td>\n",
       "      <td>0.200202</td>\n",
       "      <td>0.420947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>28.591657</td>\n",
       "      <td>1327.052255</td>\n",
       "      <td>261.759171</td>\n",
       "      <td>250.444679</td>\n",
       "      <td>242.720090</td>\n",
       "      <td>243.682674</td>\n",
       "      <td>229.856072</td>\n",
       "      <td>242.289306</td>\n",
       "      <td>0.142486</td>\n",
       "      <td>28.566771</td>\n",
       "      <td>...</td>\n",
       "      <td>0.414580</td>\n",
       "      <td>0.284968</td>\n",
       "      <td>0.253918</td>\n",
       "      <td>0.410333</td>\n",
       "      <td>0.400578</td>\n",
       "      <td>0.399632</td>\n",
       "      <td>0.399985</td>\n",
       "      <td>0.399652</td>\n",
       "      <td>0.400152</td>\n",
       "      <td>0.456716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>728.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>111.000000</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>113.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1354.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>206.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>203.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>2509.000000</td>\n",
       "      <td>471.000000</td>\n",
       "      <td>420.000000</td>\n",
       "      <td>391.000000</td>\n",
       "      <td>381.000000</td>\n",
       "      <td>384.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>8106.000000</td>\n",
       "      <td>2312.000000</td>\n",
       "      <td>1865.000000</td>\n",
       "      <td>2336.000000</td>\n",
       "      <td>1685.000000</td>\n",
       "      <td>1508.000000</td>\n",
       "      <td>1595.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Level_1        Price_1           HP_1       Attack_1  \\\n",
       "count  536414.000000  536414.000000  536414.000000  536414.000000   \n",
       "mean       49.984814    1778.619337     332.488647     304.010127   \n",
       "std        28.591657    1327.052255     261.759171     250.444679   \n",
       "min         1.000000     195.000000      10.000000       5.000000   \n",
       "25%        25.000000     728.000000     130.000000     119.000000   \n",
       "50%        50.000000    1354.000000     252.000000     222.000000   \n",
       "75%        75.000000    2509.000000     471.000000     420.000000   \n",
       "max        99.000000    8106.000000    2312.000000    1865.000000   \n",
       "\n",
       "           Defense_1       Sp_Atk_1       Sp_Def_1        Speed_1  \\\n",
       "count  536414.000000  536414.000000  536414.000000  536414.000000   \n",
       "mean      287.065308     283.583793     280.939722     290.531740   \n",
       "std       242.720090     243.682674     229.856072     242.289306   \n",
       "min         5.000000      15.000000      20.000000      15.000000   \n",
       "25%       111.000000     109.000000     110.000000     113.000000   \n",
       "50%       206.000000     200.000000     203.000000     208.000000   \n",
       "75%       391.000000     381.000000     384.000000     395.000000   \n",
       "max      2336.000000    1685.000000    1508.000000    1595.000000   \n",
       "\n",
       "         Legendary_1        Level_2  ...       Poison_2      Psychic_2  \\\n",
       "count  536414.000000  536414.000000  ...  536414.000000  536414.000000   \n",
       "mean        0.020732      49.993829  ...       0.220494       0.089155   \n",
       "std         0.142486      28.566771  ...       0.414580       0.284968   \n",
       "min         0.000000       1.000000  ...       0.000000       0.000000   \n",
       "25%         0.000000      25.000000  ...       0.000000       0.000000   \n",
       "50%         0.000000      50.000000  ...       0.000000       0.000000   \n",
       "75%         0.000000      75.000000  ...       0.000000       0.000000   \n",
       "max         1.000000      99.000000  ...       1.000000       1.000000   \n",
       "\n",
       "              Rock_2        Water_2          Night           Rain  \\\n",
       "count  536414.000000  536414.000000  536414.000000  536414.000000   \n",
       "mean        0.069273       0.214295       0.200772       0.199510   \n",
       "std         0.253918       0.410333       0.400578       0.399632   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "            Sunshine        Unknown          Windy         HPPR_1  \n",
       "count  536414.000000  536414.000000  536414.000000  536414.000000  \n",
       "mean        0.199980       0.199536       0.200202       0.420947  \n",
       "std         0.399985       0.399652       0.400152       0.456716  \n",
       "min         0.000000       0.000000       0.000000       0.000000  \n",
       "25%         0.000000       0.000000       0.000000       0.000000  \n",
       "50%         0.000000       0.000000       0.000000       0.000000  \n",
       "75%         0.000000       0.000000       0.000000       1.000000  \n",
       "max         1.000000       1.000000       1.000000       1.000000  \n",
       "\n",
       "[8 rows x 63 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_battles.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Level_1</th>\n",
       "      <th>Price_1</th>\n",
       "      <th>HP_1</th>\n",
       "      <th>Attack_1</th>\n",
       "      <th>Defense_1</th>\n",
       "      <th>Sp_Atk_1</th>\n",
       "      <th>Sp_Def_1</th>\n",
       "      <th>Speed_1</th>\n",
       "      <th>Legendary_1</th>\n",
       "      <th>Level_2</th>\n",
       "      <th>...</th>\n",
       "      <th>Poison_2</th>\n",
       "      <th>Psychic_2</th>\n",
       "      <th>Rock_2</th>\n",
       "      <th>Water_2</th>\n",
       "      <th>Night</th>\n",
       "      <th>Rain</th>\n",
       "      <th>Sunshine</th>\n",
       "      <th>Unknown</th>\n",
       "      <th>Windy</th>\n",
       "      <th>HPPR_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>268208.000000</td>\n",
       "      <td>268208.000000</td>\n",
       "      <td>268208.000000</td>\n",
       "      <td>268208.000000</td>\n",
       "      <td>268208.000000</td>\n",
       "      <td>268208.000000</td>\n",
       "      <td>268208.000000</td>\n",
       "      <td>268208.000000</td>\n",
       "      <td>268208.000000</td>\n",
       "      <td>268208.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>268208.000000</td>\n",
       "      <td>268208.000000</td>\n",
       "      <td>268208.000000</td>\n",
       "      <td>268208.000000</td>\n",
       "      <td>268208.000000</td>\n",
       "      <td>268208.000000</td>\n",
       "      <td>268208.000000</td>\n",
       "      <td>268208.000000</td>\n",
       "      <td>268208.000000</td>\n",
       "      <td>268208.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>50.041811</td>\n",
       "      <td>1779.578018</td>\n",
       "      <td>333.187504</td>\n",
       "      <td>304.207507</td>\n",
       "      <td>287.110757</td>\n",
       "      <td>283.524257</td>\n",
       "      <td>280.968838</td>\n",
       "      <td>290.579155</td>\n",
       "      <td>0.021140</td>\n",
       "      <td>50.033783</td>\n",
       "      <td>...</td>\n",
       "      <td>0.219375</td>\n",
       "      <td>0.089658</td>\n",
       "      <td>0.069308</td>\n",
       "      <td>0.214117</td>\n",
       "      <td>0.198991</td>\n",
       "      <td>0.200240</td>\n",
       "      <td>0.199319</td>\n",
       "      <td>0.201351</td>\n",
       "      <td>0.200098</td>\n",
       "      <td>0.420520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>28.564028</td>\n",
       "      <td>1328.896733</td>\n",
       "      <td>262.474662</td>\n",
       "      <td>251.098352</td>\n",
       "      <td>242.290139</td>\n",
       "      <td>243.091525</td>\n",
       "      <td>230.304347</td>\n",
       "      <td>241.968101</td>\n",
       "      <td>0.143852</td>\n",
       "      <td>28.588714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.413824</td>\n",
       "      <td>0.285692</td>\n",
       "      <td>0.253978</td>\n",
       "      <td>0.410209</td>\n",
       "      <td>0.399242</td>\n",
       "      <td>0.400181</td>\n",
       "      <td>0.399489</td>\n",
       "      <td>0.401011</td>\n",
       "      <td>0.400075</td>\n",
       "      <td>0.456799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>730.000000</td>\n",
       "      <td>131.000000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>111.000000</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>113.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1355.000000</td>\n",
       "      <td>253.000000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>207.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>203.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>2510.000000</td>\n",
       "      <td>473.000000</td>\n",
       "      <td>421.000000</td>\n",
       "      <td>391.000000</td>\n",
       "      <td>382.000000</td>\n",
       "      <td>383.000000</td>\n",
       "      <td>396.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>8106.000000</td>\n",
       "      <td>2312.000000</td>\n",
       "      <td>1865.000000</td>\n",
       "      <td>2336.000000</td>\n",
       "      <td>1685.000000</td>\n",
       "      <td>1508.000000</td>\n",
       "      <td>1595.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Level_1        Price_1           HP_1       Attack_1  \\\n",
       "count  268208.000000  268208.000000  268208.000000  268208.000000   \n",
       "mean       50.041811    1779.578018     333.187504     304.207507   \n",
       "std        28.564028    1328.896733     262.474662     251.098352   \n",
       "min         1.000000     195.000000      10.000000       5.000000   \n",
       "25%        25.000000     730.000000     131.000000     119.000000   \n",
       "50%        50.000000    1355.000000     253.000000     222.000000   \n",
       "75%        75.000000    2510.000000     473.000000     421.000000   \n",
       "max        99.000000    8106.000000    2312.000000    1865.000000   \n",
       "\n",
       "           Defense_1       Sp_Atk_1       Sp_Def_1        Speed_1  \\\n",
       "count  268208.000000  268208.000000  268208.000000  268208.000000   \n",
       "mean      287.110757     283.524257     280.968838     290.579155   \n",
       "std       242.290139     243.091525     230.304347     241.968101   \n",
       "min         5.000000      15.000000      20.000000      15.000000   \n",
       "25%       111.000000     109.000000     110.000000     113.000000   \n",
       "50%       207.000000     200.000000     203.000000     208.000000   \n",
       "75%       391.000000     382.000000     383.000000     396.000000   \n",
       "max      2336.000000    1685.000000    1508.000000    1595.000000   \n",
       "\n",
       "         Legendary_1        Level_2  ...       Poison_2      Psychic_2  \\\n",
       "count  268208.000000  268208.000000  ...  268208.000000  268208.000000   \n",
       "mean        0.021140      50.033783  ...       0.219375       0.089658   \n",
       "std         0.143852      28.588714  ...       0.413824       0.285692   \n",
       "min         0.000000       1.000000  ...       0.000000       0.000000   \n",
       "25%         0.000000      25.000000  ...       0.000000       0.000000   \n",
       "50%         0.000000      50.000000  ...       0.000000       0.000000   \n",
       "75%         0.000000      75.000000  ...       0.000000       0.000000   \n",
       "max         1.000000      99.000000  ...       1.000000       1.000000   \n",
       "\n",
       "              Rock_2        Water_2          Night           Rain  \\\n",
       "count  268208.000000  268208.000000  268208.000000  268208.000000   \n",
       "mean        0.069308       0.214117       0.198991       0.200240   \n",
       "std         0.253978       0.410209       0.399242       0.400181   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "            Sunshine        Unknown          Windy         HPPR_1  \n",
       "count  268208.000000  268208.000000  268208.000000  268208.000000  \n",
       "mean        0.199319       0.201351       0.200098       0.420520  \n",
       "std         0.399489       0.401011       0.400075       0.456799  \n",
       "min         0.000000       0.000000       0.000000       0.000000  \n",
       "25%         0.000000       0.000000       0.000000       0.000000  \n",
       "50%         0.000000       0.000000       0.000000       0.000000  \n",
       "75%         0.000000       0.000000       0.000000       1.000000  \n",
       "max         1.000000       1.000000       1.000000       1.000000  \n",
       "\n",
       "[8 rows x 63 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_battles.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_unique_battles == len(train_battles) + len(val_battles) + len(test_battles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1914541"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_battles = train_battles.append(aside, ignore_index=True)\n",
    "len(train_battles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data preparation\n",
    "\n",
    "After splitting the data, we will now perform some final data preparation to make it usable as input for in the pytorch framework.\n",
    "\n",
    "First, we extract the labels for the individual data splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_battles.BattleResult.copy()\n",
    "y_val = val_battles.BattleResult.copy()\n",
    "y_test = test_battles.BattleResult.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we drop the information in the data that is directly related to the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop response from train/test input\n",
    "X_train = train_battles.drop(['BattleResult','HPPR_1','Name_1','Name_2','Battle_MainType'], axis=1)\n",
    "X_val = val_battles.drop(['BattleResult','HPPR_1','Name_1','Name_2','Battle_MainType'], axis=1)\n",
    "X_test = test_battles.drop(['BattleResult','HPPR_1','Name_1','Name_2','Battle_MainType'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, we standardize the continuous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "cont_cols = ['Level_1','Price_1','Attack_1', 'Defense_1', 'Sp_Atk_1', 'Sp_Def_1', 'Speed_1', 'Level_2','Price_2','Attack_2', 'Defense_2', 'Sp_Atk_2', 'Sp_Def_2', 'Speed_2']\n",
    "X_train_cont = X_train[cont_cols]\n",
    "fitted_scaler = scaler.fit(X_train_cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cont_sc = fitted_scaler.transform(X_train_cont)\n",
    "X_train[cont_cols] = X_train_cont_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_cont_sc = fitted_scaler.transform(X_val[cont_cols])\n",
    "X_val[cont_cols] = X_val_cont_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_cont_sc = fitted_scaler.transform(X_test[cont_cols])\n",
    "X_test[cont_cols] = X_test_cont_sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, we pack those information into ``torch.data.Dataset`` types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Quadro T2000'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if GPU is available and set the device accordingly\n",
    "def get_device():\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')  \n",
    "    return device\n",
    "\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that from here on we expect GPU to be available, if that is not the case \n",
    "# use torch.xxxTensor instead of torch.cuda.xxxTensor\n",
    "\n",
    "train_tensors = data_utils.TensorDataset(\n",
    "    torch.cuda.FloatTensor(np.array(X_train)), \n",
    "    torch.cuda.FloatTensor(np.array(y_train)))\n",
    "\n",
    "val_tensors = data_utils.TensorDataset(\n",
    "    torch.cuda.FloatTensor(np.array(X_val)), \n",
    "    torch.cuda.FloatTensor(np.array(y_val)))\n",
    "\n",
    "test_tensors = data_utils.TensorDataset(\n",
    "    torch.cuda.FloatTensor(np.array(X_test)), \n",
    "    torch.cuda.FloatTensor(np.array(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Model training functions\n",
    "\n",
    "In the following we will implement the required functions to actually train a ``torch.nn.Module`` on the created datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_regression_model(\n",
    "        model,\n",
    "        data_loaders_dict,\n",
    "        loss_function,\n",
    "        optimizer,\n",
    "        num_epochs=100,\n",
    "        device=None,\n",
    "        early_stopping=20,\n",
    "        output_dir=\".\",\n",
    "):\n",
    "    r\"\"\" Function to train a deep architecture on a regression task.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : torch.nn.Module\n",
    "        Model to be trained.\n",
    "\n",
    "    data_loaders_dict : dict\n",
    "        The `Dataloader`s used for the training, validation and potentially the testing of the model associated with the\n",
    "        keys ``train``, ``val``, ``test``.\n",
    "\n",
    "    loss_function : pytorch loss object\n",
    "        A pytorch compatible loss function instance that is optimized during the training.\n",
    "\n",
    "    optimizer : pytorch optimizer object\n",
    "        An optimizer that is used to optimize the loss function, i.e. an instance of one of the classes defined in\n",
    "        :py:mod:`torch.optim`.\n",
    "\n",
    "    num_epochs : int\n",
    "        The number of epochs the model is at most trained for\n",
    "\n",
    "    device : :py:class:`~torch.device.Device`\n",
    "        The device that is used for the computations.\n",
    "\n",
    "    early_stopping : int\n",
    "        The number of epochs the validation loss is supposed to be not decrease before the training is stopped even\n",
    "        if the set maximum number of training epochs is not yet reached.\n",
    "\n",
    "    output_dir : str\n",
    "        The directory, where the training results i.a. checkpoints of the trained model are stored.\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (fitted_model, fitting_history_dict) : tuple(:py:class:`~torch.nn.Module`, dict)\n",
    "        [1] The best found model during the training procedure.\n",
    "        [2] A dictionary with the kes ``train`` and ``val`` that displays the evolution of the training and validation\n",
    "        loss during the training.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize a learning rate scheduler.\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \"min\", verbose=True\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    if device is None:\n",
    "        device = get_device()\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    print(device)\n",
    "    since = time.time()\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    val_loss_history = []\n",
    "    train_loss_history = []\n",
    "    fitting_history_dict = {\"train\": train_loss_history, \"val\": val_loss_history}\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = np.infty\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch {}/{}\".format(epoch, num_epochs - 1))\n",
    "        print(\"-\" * 70)\n",
    "\n",
    "        if early_stopping_counter > early_stopping and early_stopping_counter > 0:\n",
    "            print(\n",
    "                \"Stopped training because of no improvement of the validation score for \"\n",
    "                + str(early_stopping)\n",
    "                + \" epochs.\"\n",
    "            )\n",
    "            break\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            if phase == \"train\":\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_error = 0.0\n",
    "\n",
    "            for index, data in enumerate(data_loaders_dict[phase]):\n",
    "                inputs = data[0].type(torch.FloatTensor).to(device)\n",
    "                labels = data[1].type(torch.FloatTensor).to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Enable training\n",
    "                with torch.set_grad_enabled(\n",
    "                        phase == \"train\"\n",
    "                ) and torch.autograd.set_detect_anomaly(False):\n",
    "\n",
    "                    # Forward pass and calculate loss\n",
    "                    outputs = model(inputs)\n",
    "                    batch_size = labels.size(0)\n",
    "                    loss = loss_function(\n",
    "                        outputs.view(batch_size, -1), labels.view(batch_size, -1)\n",
    "                    )\n",
    "\n",
    "                # Backpropagation of the loss during the training phase\n",
    "                if phase == \"train\":\n",
    "                    with torch.autograd.set_detect_anomaly(False):\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Compute epoch statistics.\n",
    "                running_loss += loss.item()\n",
    "                running_error += np.sqrt(running_loss)\n",
    "\n",
    "            epoch_loss = running_loss / len(data_loaders_dict[phase])\n",
    "            epoch_roloss = np.sqrt(running_loss / len(data_loaders_dict[phase]))\n",
    "\n",
    "            print(\n",
    "                \"{} {} loss: {:.6f} root of loss: {:.6f}\".format(\n",
    "                    phase, loss.__class__.__name__, epoch_loss, epoch_roloss\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Deep copy the model if it has the best validation loss.\n",
    "            # Thereby the best validation loss and not the potentially requested square root of it is used to determine\n",
    "            # the superiority of a model. Due to the concavity of the square root function this has no influence\n",
    "            # on the overall process.\n",
    "            if phase == \"val\":\n",
    "                scheduler.step(epoch_loss)\n",
    "                if epoch_loss < best_loss:\n",
    "                    best_loss = epoch_loss\n",
    "                    best_model = copy.deepcopy(model)\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    torch.save(best_model, output_dir + \"/best_model.pth\")\n",
    "                    torch.save(best_model_wts, output_dir + \"/best_model_weights.pth\")\n",
    "                    early_stopping_counter = 0\n",
    "                else:\n",
    "                    early_stopping_counter += 1\n",
    "\n",
    "            fitting_history_dict[phase].append(epoch_loss)\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(\n",
    "        \"Training complete in {:.0f}m {:.0f}s\".format(\n",
    "            time_elapsed // 60, time_elapsed % 60\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(\"Best val loss : {:4f}\".format(best_loss))\n",
    "\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    # Get test loss\n",
    "    if \"test\" in data_loaders_dict.keys():\n",
    "        running_loss = 0.0\n",
    "        running_error = 0.0\n",
    "        for index, data in enumerate(data_loaders_dict[\"test\"]):\n",
    "            inputs = data[0].type(torch.FloatTensor).to(device)\n",
    "            labels = data[1].type(torch.FloatTensor).to(device)\n",
    "\n",
    "            with torch.set_grad_enabled(False):\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                outputs = outputs.view(-1)\n",
    "                labels = labels.view(-1)\n",
    "                loss = loss_function(outputs, labels)\n",
    "\n",
    "            # Compute statistics.\n",
    "            running_loss += loss.item()\n",
    "            running_error += np.sqrt(running_loss)\n",
    "\n",
    "        epoch_loss = running_loss / len(data_loaders_dict[\"test\"])\n",
    "        epoch_roloss = np.sqrt(running_loss / len(data_loaders_dict[\"test\"]))\n",
    "\n",
    "        print(\n",
    "            \"{} {} loss: {:.6f} root of loss: {:.6f}\".format(\n",
    "                \"test\", loss.__class__.__name__, epoch_loss, epoch_roloss\n",
    "            )\n",
    "        )\n",
    "        print(\"-\" * 70)\n",
    "        print(\"-\" * 70)\n",
    "\n",
    "    return model, fitting_history_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "  if type(m) == nn.Linear:\n",
    "    torch.nn.init.xavier_uniform_(m.weight)\n",
    "    m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create the dataloader and a dataloader dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data_utils.DataLoader(train_tensors, \n",
    "                                   batch_size = 1024, shuffle = True)\n",
    "\n",
    "train_eval_loader = data_utils.DataLoader(train_tensors, \n",
    "                                   batch_size = 1024, shuffle = False)\n",
    "\n",
    "val_loader = data_utils.DataLoader(val_tensors, \n",
    "                                   batch_size = 1024, shuffle = False)\n",
    "\n",
    "test_loader = data_utils.DataLoader(test_tensors, \n",
    "                                   batch_size = 1024, shuffle = False)\n",
    "\n",
    "data_loaders_dict = {'train':train_loader, 'val':val_loader, 'test':test_loader}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Experiments\n",
    "\n",
    "We have now prepared everything we need to run the first experiments. Therefore, we each time need to define a model architecture, choose a loss function, an optimizer and make decision concerning the early stopping criterium.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Small DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (dense_0): Linear(in_features=61, out_features=512, bias=True)\n",
      "  (dense_1): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (norm_1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu_1): ReLU()\n",
      "  (dense_2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (norm_2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu_2): ReLU()\n",
      "  (dense_3): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (norm_3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu_3): ReLU()\n",
      "  (dense_4): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (norm_4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu_4): ReLU()\n",
      "  (out): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1234)\n",
    "device = get_device()\n",
    "units = [512, 512, 512, 512, 512]\n",
    "\n",
    "small_dnn = nn.Sequential()\n",
    "small_dnn.add_module('dense_0',nn.Linear(X_train.shape[1], units[0]))\n",
    "for i in range(1,len(units)):\n",
    "    small_dnn.add_module('dense_{}'.format(i),nn.Linear(units[i-1], units[i]))\n",
    "    small_dnn.add_module('norm_{}'.format(i), nn.BatchNorm1d(units[i]))\n",
    "    small_dnn.add_module('relu_{}'.format(i), nn.ReLU())\n",
    "small_dnn.add_module('out', nn.Linear(units[-1],1))\n",
    "\n",
    "small_dnn.apply(init_weights)\n",
    "small_dnn.to(device)\n",
    "print(small_dnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the optimizer and the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_to_update = small_dnn.parameters()\n",
    "optimizer = AdamW(params_to_update, lr=1e-3, weight_decay=0.0)\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define the output directory and the early stopping parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = 20\n",
    "num_epochs = 200\n",
    "output_dir = '../data/99_non_catalogued/dnn_m1/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we can run the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Epoch 0/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 918.396212 root of loss: 30.305053\n",
      "val Tensor loss: 881.418357 root of loss: 29.688691\n",
      "Epoch 1/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 892.588671 root of loss: 29.876222\n",
      "val Tensor loss: 855.777579 root of loss: 29.253676\n",
      "Epoch 2/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 875.935524 root of loss: 29.596208\n",
      "val Tensor loss: 805.959714 root of loss: 28.389430\n",
      "Epoch 3/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 870.213625 root of loss: 29.499383\n",
      "val Tensor loss: 815.328986 root of loss: 28.553966\n",
      "Epoch 4/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 865.631055 root of loss: 29.421609\n",
      "val Tensor loss: 815.831488 root of loss: 28.562764\n",
      "Epoch 5/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 851.629574 root of loss: 29.182693\n",
      "val Tensor loss: 800.534581 root of loss: 28.293720\n",
      "Epoch 6/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 850.386992 root of loss: 29.161396\n",
      "val Tensor loss: 790.225772 root of loss: 28.110955\n",
      "Epoch 7/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 847.322526 root of loss: 29.108805\n",
      "val Tensor loss: 804.313527 root of loss: 28.360422\n",
      "Epoch 8/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 847.136661 root of loss: 29.105612\n",
      "val Tensor loss: 790.398486 root of loss: 28.114027\n",
      "Epoch 9/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 840.189748 root of loss: 28.986027\n",
      "val Tensor loss: 801.475344 root of loss: 28.310340\n",
      "Epoch 10/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 831.550999 root of loss: 28.836626\n",
      "val Tensor loss: 773.080757 root of loss: 27.804330\n",
      "Epoch 11/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 829.454788 root of loss: 28.800257\n",
      "val Tensor loss: 785.989310 root of loss: 28.035501\n",
      "Epoch 12/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 830.008602 root of loss: 28.809870\n",
      "val Tensor loss: 806.209219 root of loss: 28.393824\n",
      "Epoch 13/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 827.144108 root of loss: 28.760113\n",
      "val Tensor loss: 759.273182 root of loss: 27.554912\n",
      "Epoch 14/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 825.025128 root of loss: 28.723251\n",
      "val Tensor loss: 761.494702 root of loss: 27.595193\n",
      "Epoch 15/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 820.378037 root of loss: 28.642242\n",
      "val Tensor loss: 766.454784 root of loss: 27.684920\n",
      "Epoch 16/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 820.239030 root of loss: 28.639815\n",
      "val Tensor loss: 753.259718 root of loss: 27.445577\n",
      "Epoch 17/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 812.082222 root of loss: 28.497056\n",
      "val Tensor loss: 763.067474 root of loss: 27.623676\n",
      "Epoch 18/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 812.845499 root of loss: 28.510445\n",
      "val Tensor loss: 782.504779 root of loss: 27.973287\n",
      "Epoch 19/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 807.680667 root of loss: 28.419723\n",
      "val Tensor loss: 753.900063 root of loss: 27.457241\n",
      "Epoch 20/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 811.225215 root of loss: 28.482016\n",
      "val Tensor loss: 798.561898 root of loss: 28.258838\n",
      "Epoch 21/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 803.899459 root of loss: 28.353121\n",
      "val Tensor loss: 748.826503 root of loss: 27.364694\n",
      "Epoch 22/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 803.876231 root of loss: 28.352711\n",
      "val Tensor loss: 751.011255 root of loss: 27.404585\n",
      "Epoch 23/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 795.510736 root of loss: 28.204800\n",
      "val Tensor loss: 762.653637 root of loss: 27.616184\n",
      "Epoch 24/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 791.439117 root of loss: 28.132528\n",
      "val Tensor loss: 756.445026 root of loss: 27.503546\n",
      "Epoch 25/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 788.978541 root of loss: 28.088762\n",
      "val Tensor loss: 733.376681 root of loss: 27.080928\n",
      "Epoch 26/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 788.955015 root of loss: 28.088343\n",
      "val Tensor loss: 748.744702 root of loss: 27.363200\n",
      "Epoch 27/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 785.113831 root of loss: 28.019883\n",
      "val Tensor loss: 741.068609 root of loss: 27.222575\n",
      "Epoch 28/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 780.077575 root of loss: 27.929869\n",
      "val Tensor loss: 754.612625 root of loss: 27.470213\n",
      "Epoch 29/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 782.366603 root of loss: 27.970817\n",
      "val Tensor loss: 745.871245 root of loss: 27.310643\n",
      "Epoch 30/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 781.187060 root of loss: 27.949724\n",
      "val Tensor loss: 764.937576 root of loss: 27.657505\n",
      "Epoch 31/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 776.557972 root of loss: 27.866790\n",
      "val Tensor loss: 764.382615 root of loss: 27.647470\n",
      "Epoch 32/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 774.464107 root of loss: 27.829195\n",
      "val Tensor loss: 746.410380 root of loss: 27.320512\n",
      "Epoch 33/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 779.966510 root of loss: 27.927881\n",
      "val Tensor loss: 825.637359 root of loss: 28.733906\n",
      "Epoch 34/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 776.318124 root of loss: 27.862486\n",
      "val Tensor loss: 739.333747 root of loss: 27.190692\n",
      "Epoch 35/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 767.209710 root of loss: 27.698551\n",
      "val Tensor loss: 752.663043 root of loss: 27.434705\n",
      "Epoch 36/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 769.615836 root of loss: 27.741951\n",
      "val Tensor loss: 744.859884 root of loss: 27.292121\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 37/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 735.473190 root of loss: 27.119609\n",
      "val Tensor loss: 711.538248 root of loss: 26.674674\n",
      "Epoch 38/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 733.686708 root of loss: 27.086652\n",
      "val Tensor loss: 729.472620 root of loss: 27.008751\n",
      "Epoch 39/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 731.004694 root of loss: 27.037098\n",
      "val Tensor loss: 708.724238 root of loss: 26.621875\n",
      "Epoch 40/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 732.957508 root of loss: 27.073188\n",
      "val Tensor loss: 710.054174 root of loss: 26.646842\n",
      "Epoch 41/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 731.445168 root of loss: 27.045243\n",
      "val Tensor loss: 725.771826 root of loss: 26.940153\n",
      "Epoch 42/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 728.742825 root of loss: 26.995237\n",
      "val Tensor loss: 715.711820 root of loss: 26.752791\n",
      "Epoch 43/99\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Tensor loss: 734.597702 root of loss: 27.103463\n",
      "val Tensor loss: 707.266001 root of loss: 26.594473\n",
      "Epoch 44/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 725.495372 root of loss: 26.935021\n",
      "val Tensor loss: 704.879787 root of loss: 26.549572\n",
      "Epoch 45/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 732.125772 root of loss: 27.057823\n",
      "val Tensor loss: 721.920574 root of loss: 26.868580\n",
      "Epoch 46/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 730.191383 root of loss: 27.022054\n",
      "val Tensor loss: 716.390550 root of loss: 26.765473\n",
      "Epoch 47/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 727.848415 root of loss: 26.978666\n",
      "val Tensor loss: 718.681141 root of loss: 26.808229\n",
      "Epoch 48/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 722.809026 root of loss: 26.885108\n",
      "val Tensor loss: 716.492464 root of loss: 26.767377\n",
      "Epoch 49/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 724.676297 root of loss: 26.919812\n",
      "val Tensor loss: 703.842466 root of loss: 26.530030\n",
      "Epoch 50/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 730.669732 root of loss: 27.030903\n",
      "val Tensor loss: 730.605998 root of loss: 27.029724\n",
      "Epoch 51/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 733.412341 root of loss: 27.081587\n",
      "val Tensor loss: 716.635726 root of loss: 26.770053\n",
      "Epoch 52/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 724.934923 root of loss: 26.924616\n",
      "val Tensor loss: 707.597885 root of loss: 26.600712\n",
      "Epoch 53/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 726.953620 root of loss: 26.962077\n",
      "val Tensor loss: 714.309610 root of loss: 26.726571\n",
      "Epoch 54/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 720.748673 root of loss: 26.846763\n",
      "val Tensor loss: 705.281194 root of loss: 26.557131\n",
      "Epoch 55/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 724.785899 root of loss: 26.921848\n",
      "val Tensor loss: 752.335392 root of loss: 27.428733\n",
      "Epoch 56/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 719.745170 root of loss: 26.828067\n",
      "val Tensor loss: 707.308063 root of loss: 26.595264\n",
      "Epoch 57/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 726.888476 root of loss: 26.960869\n",
      "val Tensor loss: 712.516698 root of loss: 26.693008\n",
      "Epoch 58/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 727.966268 root of loss: 26.980850\n",
      "val Tensor loss: 711.696394 root of loss: 26.677638\n",
      "Epoch 59/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 720.354791 root of loss: 26.839426\n",
      "val Tensor loss: 707.416615 root of loss: 26.597305\n",
      "Epoch 60/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 726.452771 root of loss: 26.952788\n",
      "val Tensor loss: 706.876840 root of loss: 26.587156\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 61/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 722.229447 root of loss: 26.874327\n",
      "val Tensor loss: 701.977285 root of loss: 26.494854\n",
      "Epoch 62/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 719.326376 root of loss: 26.820261\n",
      "val Tensor loss: 702.685640 root of loss: 26.508218\n",
      "Epoch 63/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 725.275976 root of loss: 26.930948\n",
      "val Tensor loss: 705.242555 root of loss: 26.556403\n",
      "Epoch 64/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 717.400077 root of loss: 26.784325\n",
      "val Tensor loss: 707.261120 root of loss: 26.594381\n",
      "Epoch 65/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 725.522195 root of loss: 26.935519\n",
      "val Tensor loss: 707.638853 root of loss: 26.601482\n",
      "Epoch 66/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 717.423979 root of loss: 26.784771\n",
      "val Tensor loss: 700.060491 root of loss: 26.458656\n",
      "Epoch 67/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 714.937415 root of loss: 26.738314\n",
      "val Tensor loss: 701.605662 root of loss: 26.487840\n",
      "Epoch 68/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 716.490139 root of loss: 26.767333\n",
      "val Tensor loss: 701.521486 root of loss: 26.486251\n",
      "Epoch 69/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 722.124958 root of loss: 26.872383\n",
      "val Tensor loss: 707.775356 root of loss: 26.604048\n",
      "Epoch 70/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 722.765927 root of loss: 26.884306\n",
      "val Tensor loss: 711.087152 root of loss: 26.666217\n",
      "Epoch 71/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 721.347057 root of loss: 26.857905\n",
      "val Tensor loss: 717.195367 root of loss: 26.780503\n",
      "Epoch 72/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 719.026436 root of loss: 26.814668\n",
      "val Tensor loss: 702.430936 root of loss: 26.503414\n",
      "Epoch 73/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 721.352099 root of loss: 26.857999\n",
      "val Tensor loss: 708.216660 root of loss: 26.612340\n",
      "Epoch 74/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 718.089970 root of loss: 26.797201\n",
      "val Tensor loss: 700.867947 root of loss: 26.473911\n",
      "Epoch 75/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 715.474250 root of loss: 26.748350\n",
      "val Tensor loss: 715.491952 root of loss: 26.748681\n",
      "Epoch 76/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 715.492789 root of loss: 26.748697\n",
      "val Tensor loss: 717.613809 root of loss: 26.788315\n",
      "Epoch 77/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 724.494107 root of loss: 26.916428\n",
      "val Tensor loss: 703.340082 root of loss: 26.520560\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 78/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 719.918686 root of loss: 26.831300\n",
      "val Tensor loss: 708.366616 root of loss: 26.615158\n",
      "Epoch 79/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 718.812443 root of loss: 26.810678\n",
      "val Tensor loss: 705.481073 root of loss: 26.560894\n",
      "Epoch 80/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 723.180824 root of loss: 26.892022\n",
      "val Tensor loss: 718.815648 root of loss: 26.810738\n",
      "Epoch 81/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 718.020215 root of loss: 26.795899\n",
      "val Tensor loss: 708.151064 root of loss: 26.611108\n",
      "Epoch 82/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 719.460788 root of loss: 26.822766\n",
      "val Tensor loss: 711.383671 root of loss: 26.671777\n",
      "Epoch 83/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 720.240734 root of loss: 26.837301\n",
      "val Tensor loss: 700.234464 root of loss: 26.461944\n",
      "Epoch 84/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 716.736118 root of loss: 26.771928\n",
      "val Tensor loss: 707.784362 root of loss: 26.604217\n",
      "Epoch 85/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 722.600159 root of loss: 26.881223\n",
      "val Tensor loss: 722.619351 root of loss: 26.881580\n",
      "Epoch 86/99\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Tensor loss: 716.025520 root of loss: 26.758653\n",
      "val Tensor loss: 705.262181 root of loss: 26.556773\n",
      "Epoch 87/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 721.935626 root of loss: 26.868860\n",
      "val Tensor loss: 706.263684 root of loss: 26.575622\n",
      "Epoch 88/99\n",
      "----------------------------------------------------------------------\n",
      "Stopped training because of no improvement of the validation score for 20 epochs.\n",
      "Training complete in 41m 23s\n",
      "Best val loss : 700.060491\n",
      "test Tensor loss: 696.601187 root of loss: 26.393203\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "fitted_model, fitting_history = train_regression_model(\n",
    "        model=small_dnn,\n",
    "        data_loaders_dict=data_loaders_dict,\n",
    "        loss_function=loss_function,\n",
    "        optimizer=optimizer,\n",
    "        num_epochs=100,\n",
    "        device=None,\n",
    "        early_stopping=early_stopping,\n",
    "        output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 5.2. Medium DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (dense_0): Linear(in_features=61, out_features=512, bias=True)\n",
      "  (dense_1): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (norm_1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu_1): ReLU()\n",
      "  (dense_2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (norm_2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu_2): ReLU()\n",
      "  (dense_3): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (norm_3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu_3): ReLU()\n",
      "  (dense_4): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (norm_4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu_4): ReLU()\n",
      "  (dense_5): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (norm_5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu_5): ReLU()\n",
      "  (dense_6): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (norm_6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu_6): ReLU()\n",
      "  (out): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1234)\n",
    "device = get_device()\n",
    "units = [512, 512, 512, 512, 512, 512, 512]\n",
    "\n",
    "medium_dnn = nn.Sequential()\n",
    "medium_dnn.add_module('dense_0',nn.Linear(X_train.shape[1], units[0]))\n",
    "for i in range(1,len(units)):\n",
    "    medium_dnn.add_module('dense_{}'.format(i),nn.Linear(units[i-1], units[i]))\n",
    "    medium_dnn.add_module('norm_{}'.format(i), nn.BatchNorm1d(units[i]))\n",
    "    medium_dnn.add_module('relu_{}'.format(i), nn.ReLU())\n",
    "medium_dnn.add_module('out', nn.Linear(units[-1],1))\n",
    "\n",
    "medium_dnn.apply(init_weights)\n",
    "medium_dnn.to(device)\n",
    "print(medium_dnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the optimizer and the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_to_update = medium_dnn.parameters()\n",
    "optimizer = AdamW(params_to_update, lr=1e-3, weight_decay=0.0)\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define the output directory and the early stopping parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = 20\n",
    "num_epochs = 100\n",
    "output_dir = '../data/99_non_catalogued/dnn_m2/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we can run the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Epoch 0/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1102.872542 root of loss: 33.209525\n",
      "val Tensor loss: 1160.817235 root of loss: 34.070768\n",
      "Epoch 1/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1073.195569 root of loss: 32.759664\n",
      "val Tensor loss: 2454.692765 root of loss: 49.544856\n",
      "Epoch 2/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1078.732792 root of loss: 32.844068\n",
      "val Tensor loss: 1393.757479 root of loss: 37.333061\n",
      "Epoch 3/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1067.042490 root of loss: 32.665616\n",
      "val Tensor loss: 1043.968571 root of loss: 32.310502\n",
      "Epoch 4/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1065.507997 root of loss: 32.642120\n",
      "val Tensor loss: 953.825943 root of loss: 30.884073\n",
      "Epoch 5/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1036.436730 root of loss: 32.193737\n",
      "val Tensor loss: 1083.385359 root of loss: 32.914820\n",
      "Epoch 6/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1044.521048 root of loss: 32.319051\n",
      "val Tensor loss: 1497.225156 root of loss: 38.693994\n",
      "Epoch 7/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1030.864516 root of loss: 32.107079\n",
      "val Tensor loss: 1022.961102 root of loss: 31.983763\n",
      "Epoch 8/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1027.232083 root of loss: 32.050462\n",
      "val Tensor loss: 1083.259467 root of loss: 32.912907\n",
      "Epoch 9/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1030.579657 root of loss: 32.102643\n",
      "val Tensor loss: 1059.046882 root of loss: 32.543001\n",
      "Epoch 10/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1036.333060 root of loss: 32.192127\n",
      "val Tensor loss: 977.190835 root of loss: 31.260052\n",
      "Epoch 11/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 999.007869 root of loss: 31.607086\n",
      "val Tensor loss: 1343.979696 root of loss: 36.660329\n",
      "Epoch 12/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1003.460855 root of loss: 31.677450\n",
      "val Tensor loss: 973.046208 root of loss: 31.193689\n",
      "Epoch 13/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1008.688650 root of loss: 31.759859\n",
      "val Tensor loss: 1162.356161 root of loss: 34.093345\n",
      "Epoch 14/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 987.195210 root of loss: 31.419663\n",
      "val Tensor loss: 1314.566076 root of loss: 36.256945\n",
      "Epoch 15/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 994.093091 root of loss: 31.529242\n",
      "val Tensor loss: 940.026022 root of loss: 30.659844\n",
      "Epoch 16/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 952.831496 root of loss: 30.867969\n",
      "val Tensor loss: 971.876181 root of loss: 31.174929\n",
      "Epoch 17/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 962.157797 root of loss: 31.018669\n",
      "val Tensor loss: 1064.272639 root of loss: 32.623192\n",
      "Epoch 18/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 950.490396 root of loss: 30.830024\n",
      "val Tensor loss: 828.184276 root of loss: 28.778191\n",
      "Epoch 19/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 946.302322 root of loss: 30.762027\n",
      "val Tensor loss: 1093.172862 root of loss: 33.063165\n",
      "Epoch 20/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 948.114320 root of loss: 30.791465\n",
      "val Tensor loss: 916.851888 root of loss: 30.279562\n",
      "Epoch 21/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 920.562962 root of loss: 30.340781\n",
      "val Tensor loss: 889.825626 root of loss: 29.829945\n",
      "Epoch 22/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 939.011814 root of loss: 30.643300\n",
      "val Tensor loss: 1637.081270 root of loss: 40.460861\n",
      "Epoch 23/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 918.285609 root of loss: 30.303228\n",
      "val Tensor loss: 968.964670 root of loss: 31.128197\n",
      "Epoch 24/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 918.272089 root of loss: 30.303005\n",
      "val Tensor loss: 1201.849388 root of loss: 34.667699\n",
      "Epoch 25/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 928.923643 root of loss: 30.478249\n",
      "val Tensor loss: 1134.157851 root of loss: 33.677260\n",
      "Epoch 26/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 924.731628 root of loss: 30.409400\n",
      "val Tensor loss: 999.035752 root of loss: 31.607527\n",
      "Epoch 27/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 907.283013 root of loss: 30.121139\n",
      "val Tensor loss: 1172.349089 root of loss: 34.239584\n",
      "Epoch 28/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 893.177130 root of loss: 29.886069\n",
      "val Tensor loss: 949.009878 root of loss: 30.806004\n",
      "Epoch 29/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 885.173680 root of loss: 29.751869\n",
      "val Tensor loss: 1369.140455 root of loss: 37.001898\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 30/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 657.778216 root of loss: 25.647187\n",
      "val Tensor loss: 673.328476 root of loss: 25.948574\n",
      "Epoch 31/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 633.517336 root of loss: 25.169770\n",
      "val Tensor loss: 669.449951 root of loss: 25.873731\n",
      "Epoch 32/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 629.603747 root of loss: 25.091906\n",
      "val Tensor loss: 662.451274 root of loss: 25.738129\n",
      "Epoch 33/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 623.212963 root of loss: 24.964234\n",
      "val Tensor loss: 717.433100 root of loss: 26.784942\n",
      "Epoch 34/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 617.050216 root of loss: 24.840495\n",
      "val Tensor loss: 670.391209 root of loss: 25.891914\n",
      "Epoch 35/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 602.167250 root of loss: 24.539096\n",
      "val Tensor loss: 657.554532 root of loss: 25.642826\n",
      "Epoch 36/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 610.923076 root of loss: 24.716858\n",
      "val Tensor loss: 649.054974 root of loss: 25.476557\n",
      "Epoch 37/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 601.053950 root of loss: 24.516402\n",
      "val Tensor loss: 664.532811 root of loss: 25.778534\n",
      "Epoch 38/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 608.117270 root of loss: 24.660034\n",
      "val Tensor loss: 676.551142 root of loss: 26.010597\n",
      "Epoch 39/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 601.163745 root of loss: 24.518641\n",
      "val Tensor loss: 685.038487 root of loss: 26.173240\n",
      "Epoch 40/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 595.968685 root of loss: 24.412470\n",
      "val Tensor loss: 697.474750 root of loss: 26.409747\n",
      "Epoch 41/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 599.284417 root of loss: 24.480286\n",
      "val Tensor loss: 646.044609 root of loss: 25.417408\n",
      "Epoch 42/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 587.905478 root of loss: 24.246762\n",
      "val Tensor loss: 660.809114 root of loss: 25.706208\n",
      "Epoch 43/99\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Tensor loss: 585.567028 root of loss: 24.198492\n",
      "val Tensor loss: 695.386746 root of loss: 26.370187\n",
      "Epoch 44/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 587.945257 root of loss: 24.247582\n",
      "val Tensor loss: 638.677491 root of loss: 25.272069\n",
      "Epoch 45/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 583.685204 root of loss: 24.159578\n",
      "val Tensor loss: 689.181064 root of loss: 26.252258\n",
      "Epoch 46/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 581.719075 root of loss: 24.118853\n",
      "val Tensor loss: 661.066207 root of loss: 25.711208\n",
      "Epoch 47/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 586.578452 root of loss: 24.219382\n",
      "val Tensor loss: 660.617697 root of loss: 25.702484\n",
      "Epoch 48/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 572.955901 root of loss: 23.936497\n",
      "val Tensor loss: 661.899016 root of loss: 25.727398\n",
      "Epoch 49/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 581.128105 root of loss: 24.106599\n",
      "val Tensor loss: 711.717488 root of loss: 26.678034\n",
      "Epoch 50/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 579.113397 root of loss: 24.064775\n",
      "val Tensor loss: 718.141977 root of loss: 26.798171\n",
      "Epoch 51/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 580.720416 root of loss: 24.098141\n",
      "val Tensor loss: 650.102331 root of loss: 25.497104\n",
      "Epoch 52/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 572.419002 root of loss: 23.925280\n",
      "val Tensor loss: 662.582494 root of loss: 25.740678\n",
      "Epoch 53/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 572.355273 root of loss: 23.923948\n",
      "val Tensor loss: 661.810926 root of loss: 25.725686\n",
      "Epoch 54/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 566.933855 root of loss: 23.810373\n",
      "val Tensor loss: 654.336651 root of loss: 25.580005\n",
      "Epoch 55/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 563.443451 root of loss: 23.736964\n",
      "val Tensor loss: 665.001176 root of loss: 25.787617\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 56/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 543.068572 root of loss: 23.303832\n",
      "val Tensor loss: 648.285768 root of loss: 25.461457\n",
      "Epoch 57/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 538.420279 root of loss: 23.203885\n",
      "val Tensor loss: 653.292226 root of loss: 25.559582\n",
      "Epoch 58/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 539.072552 root of loss: 23.217936\n",
      "val Tensor loss: 649.937720 root of loss: 25.493876\n",
      "Epoch 59/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 542.309567 root of loss: 23.287541\n",
      "val Tensor loss: 668.994188 root of loss: 25.864922\n",
      "Epoch 60/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 540.537220 root of loss: 23.249456\n",
      "val Tensor loss: 669.655653 root of loss: 25.877706\n",
      "Epoch 61/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 546.412114 root of loss: 23.375460\n",
      "val Tensor loss: 621.670859 root of loss: 24.933328\n",
      "Epoch 62/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 538.198273 root of loss: 23.199101\n",
      "val Tensor loss: 618.124467 root of loss: 24.862109\n",
      "Epoch 63/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 544.258214 root of loss: 23.329342\n",
      "val Tensor loss: 621.935876 root of loss: 24.938642\n",
      "Epoch 64/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 540.648339 root of loss: 23.251846\n",
      "val Tensor loss: 628.548970 root of loss: 25.070879\n",
      "Epoch 65/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 538.967933 root of loss: 23.215683\n",
      "val Tensor loss: 642.639713 root of loss: 25.350339\n",
      "Epoch 66/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 538.754612 root of loss: 23.211088\n",
      "val Tensor loss: 628.811834 root of loss: 25.076121\n",
      "Epoch 67/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 536.004707 root of loss: 23.151775\n",
      "val Tensor loss: 648.474025 root of loss: 25.465153\n",
      "Epoch 68/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 538.763967 root of loss: 23.211290\n",
      "val Tensor loss: 678.233178 root of loss: 26.042910\n",
      "Epoch 69/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 539.109499 root of loss: 23.218732\n",
      "val Tensor loss: 618.594454 root of loss: 24.871559\n",
      "Epoch 70/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 535.602279 root of loss: 23.143083\n",
      "val Tensor loss: 630.362728 root of loss: 25.107025\n",
      "Epoch 71/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 537.025439 root of loss: 23.173809\n",
      "val Tensor loss: 644.247659 root of loss: 25.382034\n",
      "Epoch 72/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 537.075622 root of loss: 23.174892\n",
      "val Tensor loss: 630.906470 root of loss: 25.117852\n",
      "Epoch 73/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 536.421124 root of loss: 23.160767\n",
      "val Tensor loss: 660.971604 root of loss: 25.709368\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 74/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 534.972517 root of loss: 23.129473\n",
      "val Tensor loss: 634.765640 root of loss: 25.194556\n",
      "Epoch 75/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 533.153011 root of loss: 23.090106\n",
      "val Tensor loss: 644.488868 root of loss: 25.386785\n",
      "Epoch 76/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 535.364163 root of loss: 23.137938\n",
      "val Tensor loss: 639.098885 root of loss: 25.280405\n",
      "Epoch 77/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 537.593502 root of loss: 23.186063\n",
      "val Tensor loss: 638.606392 root of loss: 25.270663\n",
      "Epoch 78/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 539.233044 root of loss: 23.221392\n",
      "val Tensor loss: 653.158155 root of loss: 25.556959\n",
      "Epoch 79/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 536.638552 root of loss: 23.165460\n",
      "val Tensor loss: 647.179843 root of loss: 25.439730\n",
      "Epoch 80/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 531.830063 root of loss: 23.061441\n",
      "val Tensor loss: 627.217290 root of loss: 25.044307\n",
      "Epoch 81/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 534.483505 root of loss: 23.118899\n",
      "val Tensor loss: 684.235134 root of loss: 26.157889\n",
      "Epoch 82/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 537.277259 root of loss: 23.179242\n",
      "val Tensor loss: 638.690186 root of loss: 25.272321\n",
      "Epoch 83/99\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 535.603993 root of loss: 23.143120\n",
      "val Tensor loss: 650.009402 root of loss: 25.495282\n",
      "Epoch 84/99\n",
      "----------------------------------------------------------------------\n",
      "Stopped training because of no improvement of the validation score for 20 epochs.\n",
      "Training complete in 49m 53s\n",
      "Best val loss : 618.124467\n",
      "test Tensor loss: 632.073385 root of loss: 25.141070\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "fitted_model, fitting_history = train_regression_model(\n",
    "        model=medium_dnn,\n",
    "        data_loaders_dict=data_loaders_dict,\n",
    "        loss_function=loss_function,\n",
    "        optimizer=optimizer,\n",
    "        num_epochs=num_epochs,\n",
    "        device=None,\n",
    "        early_stopping=early_stopping,\n",
    "        output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 5.3. Tiny DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (dense_0): Linear(in_features=61, out_features=512, bias=True)\n",
      "  (dense_1): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (norm_1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu_1): ReLU()\n",
      "  (dense_2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (norm_2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu_2): ReLU()\n",
      "  (dense_3): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (norm_3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu_3): ReLU()\n",
      "  (out): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1234)\n",
    "device = get_device()\n",
    "units = [512, 512, 512, 512]\n",
    "\n",
    "tiny_dnn = nn.Sequential()\n",
    "tiny_dnn.add_module('dense_0',nn.Linear(X_train.shape[1], units[0]))\n",
    "for i in range(1,len(units)):\n",
    "    tiny_dnn.add_module('dense_{}'.format(i),nn.Linear(units[i-1], units[i]))\n",
    "    tiny_dnn.add_module('norm_{}'.format(i), nn.BatchNorm1d(units[i]))\n",
    "    tiny_dnn.add_module('relu_{}'.format(i), nn.ReLU())\n",
    "tiny_dnn.add_module('out', nn.Linear(units[-1],1))\n",
    "\n",
    "tiny_dnn.apply(init_weights)\n",
    "tiny_dnn.to(device)\n",
    "print(tiny_dnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the optimizer and the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_to_update = tiny_dnn.parameters()\n",
    "optimizer = AdamW(params_to_update, lr=1e-3, weight_decay=0.0)\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define the output directory and the early stopping parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = 20\n",
    "num_epochs = 200\n",
    "output_dir = '../data/99_non_catalogued/dnn_m3/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we can run the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Epoch 0/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 38221.495313 root of loss: 195.503185\n",
      "val Tensor loss: 5500.130947 root of loss: 74.162868\n",
      "Epoch 1/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 3827.354630 root of loss: 61.865618\n",
      "val Tensor loss: 4592.416506 root of loss: 67.767371\n",
      "Epoch 2/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 3296.990182 root of loss: 57.419423\n",
      "val Tensor loss: 7739.951804 root of loss: 87.976996\n",
      "Epoch 3/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 2936.769357 root of loss: 54.191968\n",
      "val Tensor loss: 2998.952043 root of loss: 54.762688\n",
      "Epoch 4/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 2623.798537 root of loss: 51.223027\n",
      "val Tensor loss: 2389.925378 root of loss: 48.886863\n",
      "Epoch 5/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 2389.913066 root of loss: 48.886737\n",
      "val Tensor loss: 1950.078938 root of loss: 44.159698\n",
      "Epoch 6/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 2271.151958 root of loss: 47.656605\n",
      "val Tensor loss: 2174.516945 root of loss: 46.631716\n",
      "Epoch 7/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 2211.522923 root of loss: 47.026832\n",
      "val Tensor loss: 2289.492878 root of loss: 47.848646\n",
      "Epoch 8/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 2087.693782 root of loss: 45.691288\n",
      "val Tensor loss: 3020.832369 root of loss: 54.962099\n",
      "Epoch 9/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 2054.041785 root of loss: 45.321538\n",
      "val Tensor loss: 2435.287058 root of loss: 49.348628\n",
      "Epoch 10/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1965.138731 root of loss: 44.329885\n",
      "val Tensor loss: 2884.907239 root of loss: 53.711332\n",
      "Epoch 11/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1904.500721 root of loss: 43.640586\n",
      "val Tensor loss: 2263.524565 root of loss: 47.576513\n",
      "Epoch 12/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1835.396697 root of loss: 42.841530\n",
      "val Tensor loss: 1754.281927 root of loss: 41.884149\n",
      "Epoch 13/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1845.718021 root of loss: 42.961821\n",
      "val Tensor loss: 1622.814368 root of loss: 40.284170\n",
      "Epoch 14/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1783.002947 root of loss: 42.225620\n",
      "val Tensor loss: 1634.035666 root of loss: 40.423207\n",
      "Epoch 15/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1759.217873 root of loss: 41.943031\n",
      "val Tensor loss: 1346.302003 root of loss: 36.691988\n",
      "Epoch 16/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1754.912255 root of loss: 41.891673\n",
      "val Tensor loss: 1380.192446 root of loss: 37.150941\n",
      "Epoch 17/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1681.872379 root of loss: 41.010637\n",
      "val Tensor loss: 1668.143726 root of loss: 40.842915\n",
      "Epoch 18/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1672.723144 root of loss: 40.898938\n",
      "val Tensor loss: 2016.221740 root of loss: 44.902358\n",
      "Epoch 19/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1657.335074 root of loss: 40.710380\n",
      "val Tensor loss: 1929.077036 root of loss: 43.921260\n",
      "Epoch 20/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1604.879112 root of loss: 40.060942\n",
      "val Tensor loss: 1419.092042 root of loss: 37.670838\n",
      "Epoch 21/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1583.715462 root of loss: 39.795923\n",
      "val Tensor loss: 1384.934009 root of loss: 37.214702\n",
      "Epoch 22/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1614.898713 root of loss: 40.185802\n",
      "val Tensor loss: 1357.022823 root of loss: 36.837791\n",
      "Epoch 23/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1595.792886 root of loss: 39.947376\n",
      "val Tensor loss: 3566.548993 root of loss: 59.720591\n",
      "Epoch 24/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1526.936346 root of loss: 39.076033\n",
      "val Tensor loss: 1348.847741 root of loss: 36.726663\n",
      "Epoch 25/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1543.055035 root of loss: 39.281739\n",
      "val Tensor loss: 1423.276019 root of loss: 37.726331\n",
      "Epoch 26/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1519.182507 root of loss: 38.976692\n",
      "val Tensor loss: 1867.041224 root of loss: 43.209272\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 27/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1241.303783 root of loss: 35.232141\n",
      "val Tensor loss: 929.751419 root of loss: 30.491825\n",
      "Epoch 28/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1221.597180 root of loss: 34.951354\n",
      "val Tensor loss: 941.789356 root of loss: 30.688587\n",
      "Epoch 29/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1221.614152 root of loss: 34.951597\n",
      "val Tensor loss: 948.815947 root of loss: 30.802856\n",
      "Epoch 30/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1215.928869 root of loss: 34.870172\n",
      "val Tensor loss: 956.261762 root of loss: 30.923482\n",
      "Epoch 31/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1192.026369 root of loss: 34.525735\n",
      "val Tensor loss: 911.156347 root of loss: 30.185366\n",
      "Epoch 32/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1185.927363 root of loss: 34.437296\n",
      "val Tensor loss: 928.484480 root of loss: 30.471043\n",
      "Epoch 33/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1183.760271 root of loss: 34.405817\n",
      "val Tensor loss: 910.286160 root of loss: 30.170949\n",
      "Epoch 34/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1183.506888 root of loss: 34.402135\n",
      "val Tensor loss: 930.606191 root of loss: 30.505839\n",
      "Epoch 35/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1174.809074 root of loss: 34.275488\n",
      "val Tensor loss: 954.621264 root of loss: 30.896946\n",
      "Epoch 36/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1181.389703 root of loss: 34.371350\n",
      "val Tensor loss: 916.767318 root of loss: 30.278166\n",
      "Epoch 37/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1168.843878 root of loss: 34.188359\n",
      "val Tensor loss: 916.292929 root of loss: 30.270331\n",
      "Epoch 38/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1166.649346 root of loss: 34.156249\n",
      "val Tensor loss: 918.463524 root of loss: 30.306163\n",
      "Epoch 39/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1156.208121 root of loss: 34.003060\n",
      "val Tensor loss: 943.038124 root of loss: 30.708926\n",
      "Epoch 40/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1143.242605 root of loss: 33.811871\n",
      "val Tensor loss: 891.319395 root of loss: 29.854973\n",
      "Epoch 41/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1144.705253 root of loss: 33.833493\n",
      "val Tensor loss: 873.984042 root of loss: 29.563221\n",
      "Epoch 42/199\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Tensor loss: 1142.897570 root of loss: 33.806768\n",
      "val Tensor loss: 886.337654 root of loss: 29.771423\n",
      "Epoch 43/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1146.007420 root of loss: 33.852731\n",
      "val Tensor loss: 939.979813 root of loss: 30.659090\n",
      "Epoch 44/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1129.468034 root of loss: 33.607559\n",
      "val Tensor loss: 863.827043 root of loss: 29.390935\n",
      "Epoch 45/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1136.682490 root of loss: 33.714722\n",
      "val Tensor loss: 891.712609 root of loss: 29.861557\n",
      "Epoch 46/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1124.637826 root of loss: 33.535620\n",
      "val Tensor loss: 875.905860 root of loss: 29.595707\n",
      "Epoch 47/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1115.330296 root of loss: 33.396561\n",
      "val Tensor loss: 883.840655 root of loss: 29.729458\n",
      "Epoch 48/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1114.838188 root of loss: 33.389193\n",
      "val Tensor loss: 868.925887 root of loss: 29.477549\n",
      "Epoch 49/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1107.998591 root of loss: 33.286613\n",
      "val Tensor loss: 888.908702 root of loss: 29.814572\n",
      "Epoch 50/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1113.753580 root of loss: 33.372947\n",
      "val Tensor loss: 843.067081 root of loss: 29.035617\n",
      "Epoch 51/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1104.920701 root of loss: 33.240347\n",
      "val Tensor loss: 882.518131 root of loss: 29.707207\n",
      "Epoch 52/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1111.909252 root of loss: 33.345303\n",
      "val Tensor loss: 925.444287 root of loss: 30.421116\n",
      "Epoch 53/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1096.042266 root of loss: 33.106529\n",
      "val Tensor loss: 843.020043 root of loss: 29.034807\n",
      "Epoch 54/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1107.648363 root of loss: 33.281352\n",
      "val Tensor loss: 851.175398 root of loss: 29.174910\n",
      "Epoch 55/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1089.722125 root of loss: 33.010939\n",
      "val Tensor loss: 854.743631 root of loss: 29.235999\n",
      "Epoch 56/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1087.339990 root of loss: 32.974839\n",
      "val Tensor loss: 860.902882 root of loss: 29.341147\n",
      "Epoch 57/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1083.128844 root of loss: 32.910923\n",
      "val Tensor loss: 848.519764 root of loss: 29.129363\n",
      "Epoch 58/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1084.995163 root of loss: 32.939265\n",
      "val Tensor loss: 850.074590 root of loss: 29.156039\n",
      "Epoch 59/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1081.263485 root of loss: 32.882571\n",
      "val Tensor loss: 851.369837 root of loss: 29.178243\n",
      "Epoch 60/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1071.455033 root of loss: 32.733088\n",
      "val Tensor loss: 868.016982 root of loss: 29.462128\n",
      "Epoch 61/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1078.845158 root of loss: 32.845778\n",
      "val Tensor loss: 867.392514 root of loss: 29.451528\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 62/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1025.551091 root of loss: 32.024227\n",
      "val Tensor loss: 868.330666 root of loss: 29.467451\n",
      "Epoch 63/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1028.505280 root of loss: 32.070318\n",
      "val Tensor loss: 851.779599 root of loss: 29.185263\n",
      "Epoch 64/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1030.661806 root of loss: 32.103922\n",
      "val Tensor loss: 813.201914 root of loss: 28.516695\n",
      "Epoch 65/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1027.615906 root of loss: 32.056449\n",
      "val Tensor loss: 827.490763 root of loss: 28.766139\n",
      "Epoch 66/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1019.153312 root of loss: 31.924181\n",
      "val Tensor loss: 837.136684 root of loss: 28.933314\n",
      "Epoch 67/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1011.999070 root of loss: 31.811933\n",
      "val Tensor loss: 809.328441 root of loss: 28.448698\n",
      "Epoch 68/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1006.219421 root of loss: 31.720962\n",
      "val Tensor loss: 829.132767 root of loss: 28.794666\n",
      "Epoch 69/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1015.030049 root of loss: 31.859536\n",
      "val Tensor loss: 840.887751 root of loss: 28.998065\n",
      "Epoch 70/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1013.030015 root of loss: 31.828132\n",
      "val Tensor loss: 830.249505 root of loss: 28.814050\n",
      "Epoch 71/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1015.406679 root of loss: 31.865446\n",
      "val Tensor loss: 808.107484 root of loss: 28.427231\n",
      "Epoch 72/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1015.462493 root of loss: 31.866322\n",
      "val Tensor loss: 806.421888 root of loss: 28.397568\n",
      "Epoch 73/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1017.276545 root of loss: 31.894773\n",
      "val Tensor loss: 827.812117 root of loss: 28.771724\n",
      "Epoch 74/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1011.022095 root of loss: 31.796574\n",
      "val Tensor loss: 803.409796 root of loss: 28.344484\n",
      "Epoch 75/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1020.915016 root of loss: 31.951761\n",
      "val Tensor loss: 830.193279 root of loss: 28.813075\n",
      "Epoch 76/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1029.832725 root of loss: 32.091007\n",
      "val Tensor loss: 803.830526 root of loss: 28.351905\n",
      "Epoch 77/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1038.964430 root of loss: 32.232971\n",
      "val Tensor loss: 810.857980 root of loss: 28.475568\n",
      "Epoch 78/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1019.112371 root of loss: 31.923539\n",
      "val Tensor loss: 826.774856 root of loss: 28.753693\n",
      "Epoch 79/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1016.244319 root of loss: 31.878587\n",
      "val Tensor loss: 805.027164 root of loss: 28.373001\n",
      "Epoch 80/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1025.814174 root of loss: 32.028334\n",
      "val Tensor loss: 820.496516 root of loss: 28.644310\n",
      "Epoch 81/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1018.970714 root of loss: 31.921321\n",
      "val Tensor loss: 805.722171 root of loss: 28.385246\n",
      "Epoch 82/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1010.280766 root of loss: 31.784914\n",
      "val Tensor loss: 813.033738 root of loss: 28.513746\n",
      "Epoch 83/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1023.912998 root of loss: 31.998641\n",
      "val Tensor loss: 814.527163 root of loss: 28.539922\n",
      "Epoch 84/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1031.563405 root of loss: 32.117961\n",
      "val Tensor loss: 805.664949 root of loss: 28.384238\n",
      "Epoch 85/199\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Tensor loss: 1008.208630 root of loss: 31.752301\n",
      "val Tensor loss: 805.833266 root of loss: 28.387203\n",
      "Epoch    85: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 86/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1009.012570 root of loss: 31.764958\n",
      "val Tensor loss: 814.057242 root of loss: 28.531688\n",
      "Epoch 87/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1009.458827 root of loss: 31.771982\n",
      "val Tensor loss: 816.015322 root of loss: 28.565982\n",
      "Epoch 88/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1007.013428 root of loss: 31.733475\n",
      "val Tensor loss: 819.682197 root of loss: 28.630093\n",
      "Epoch 89/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1014.045913 root of loss: 31.844088\n",
      "val Tensor loss: 801.089702 root of loss: 28.303528\n",
      "Epoch 90/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1015.936215 root of loss: 31.873754\n",
      "val Tensor loss: 810.887198 root of loss: 28.476081\n",
      "Epoch 91/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1013.643985 root of loss: 31.837776\n",
      "val Tensor loss: 899.948860 root of loss: 29.999148\n",
      "Epoch 92/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1009.730765 root of loss: 31.776261\n",
      "val Tensor loss: 809.677800 root of loss: 28.454838\n",
      "Epoch 93/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1013.626969 root of loss: 31.837509\n",
      "val Tensor loss: 800.741464 root of loss: 28.297376\n",
      "Epoch 94/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1007.078840 root of loss: 31.734506\n",
      "val Tensor loss: 804.949898 root of loss: 28.371639\n",
      "Epoch 95/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1010.944493 root of loss: 31.795353\n",
      "val Tensor loss: 838.999899 root of loss: 28.965495\n",
      "Epoch 96/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1002.180418 root of loss: 31.657233\n",
      "val Tensor loss: 803.913650 root of loss: 28.353371\n",
      "Epoch 97/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1010.745252 root of loss: 31.792220\n",
      "val Tensor loss: 806.398279 root of loss: 28.397153\n",
      "Epoch 98/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1000.615721 root of loss: 31.632511\n",
      "val Tensor loss: 801.974425 root of loss: 28.319153\n",
      "Epoch 99/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1007.868315 root of loss: 31.746942\n",
      "val Tensor loss: 805.376451 root of loss: 28.379155\n",
      "Epoch 100/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1003.770191 root of loss: 31.682332\n",
      "val Tensor loss: 816.603770 root of loss: 28.576280\n",
      "Epoch 101/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1007.520634 root of loss: 31.741466\n",
      "val Tensor loss: 800.860195 root of loss: 28.299473\n",
      "Epoch 102/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1011.891696 root of loss: 31.810245\n",
      "val Tensor loss: 824.866265 root of loss: 28.720485\n",
      "Epoch 103/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1005.236770 root of loss: 31.705469\n",
      "val Tensor loss: 840.543498 root of loss: 28.992128\n",
      "Epoch 104/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1005.161545 root of loss: 31.704283\n",
      "val Tensor loss: 813.665481 root of loss: 28.524822\n",
      "Epoch   104: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 105/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 997.068648 root of loss: 31.576394\n",
      "val Tensor loss: 834.709393 root of loss: 28.891338\n",
      "Epoch 106/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1012.263946 root of loss: 31.816096\n",
      "val Tensor loss: 806.981813 root of loss: 28.407425\n",
      "Epoch 107/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1014.260440 root of loss: 31.847456\n",
      "val Tensor loss: 809.801747 root of loss: 28.457016\n",
      "Epoch 108/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1016.233211 root of loss: 31.878413\n",
      "val Tensor loss: 862.080679 root of loss: 29.361210\n",
      "Epoch 109/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1005.300054 root of loss: 31.706467\n",
      "val Tensor loss: 804.047758 root of loss: 28.355736\n",
      "Epoch 110/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1010.621723 root of loss: 31.790277\n",
      "val Tensor loss: 800.759858 root of loss: 28.297701\n",
      "Epoch 111/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1000.164227 root of loss: 31.625373\n",
      "val Tensor loss: 832.343360 root of loss: 28.850362\n",
      "Epoch 112/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 996.313821 root of loss: 31.564439\n",
      "val Tensor loss: 835.984773 root of loss: 28.913401\n",
      "Epoch 113/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1009.516457 root of loss: 31.772889\n",
      "val Tensor loss: 827.232993 root of loss: 28.761658\n",
      "Epoch 114/199\n",
      "----------------------------------------------------------------------\n",
      "train Tensor loss: 1006.755432 root of loss: 31.729410\n",
      "val Tensor loss: 806.443501 root of loss: 28.397949\n",
      "Epoch 115/199\n",
      "----------------------------------------------------------------------\n",
      "Stopped training because of no improvement of the validation score for 20 epochs.\n",
      "Training complete in 49m 36s\n",
      "Best val loss : 800.741464\n",
      "test Tensor loss: 795.643614 root of loss: 28.207155\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "fitted_model, fitting_history = train_regression_model(\n",
    "        model=tiny_dnn,\n",
    "        data_loaders_dict=data_loaders_dict,\n",
    "        loss_function=loss_function,\n",
    "        optimizer=optimizer,\n",
    "        num_epochs=num_epochs,\n",
    "        device=None,\n",
    "        early_stopping=early_stopping,\n",
    "        output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "### 5.4. Tiny DNN (256)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "device = get_device()\n",
    "units = [256, 256, 256, 256]\n",
    "\n",
    "tiny_dnn_256 = nn.Sequential()\n",
    "tiny_dnn_256.add_module('dense_0',nn.Linear(X_train.shape[1], units[0]))\n",
    "for i in range(1,len(units)):\n",
    "    tiny_dnn_256.add_module('dense_{}'.format(i),nn.Linear(units[i-1], units[i]))\n",
    "    tiny_dnn_256.add_module('norm_{}'.format(i), nn.BatchNorm1d(units[i]))\n",
    "    tiny_dnn_256.add_module('relu_{}'.format(i), nn.ReLU())\n",
    "tiny_dnn_256.add_module('out', nn.Linear(units[-1],1))\n",
    "\n",
    "tiny_dnn_256.apply(init_weights)\n",
    "tiny_dnn_256.to(device)\n",
    "print(tiny_dnn_256)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now define the optimizer and the loss function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "params_to_update = tiny_dnn_256.parameters()\n",
    "optimizer = AdamW(params_to_update, lr=1e-3, weight_decay=0.0)\n",
    "loss_function = nn.MSELoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we define the output directory and the early stopping parameter."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "early_stopping = 20\n",
    "num_epochs = 200\n",
    "output_dir = '../data/99_non_catalogued/dnn_m4/'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And finally we can run the training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fitted_model, fitting_history = train_regression_model(\n",
    "        model=tiny_dnn,\n",
    "        data_loaders_dict=data_loaders_dict,\n",
    "        loss_function=loss_function,\n",
    "        optimizer=optimizer,\n",
    "        num_epochs=num_epochs,\n",
    "        device=None,\n",
    "        early_stopping=early_stopping,\n",
    "        output_dir=output_dir)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}